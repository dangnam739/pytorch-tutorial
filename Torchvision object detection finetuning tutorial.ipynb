{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision Object Detection Finetuning Tutorial\n",
    "\n",
    "**References**: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "For this tutorial, we will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "## Define the Dataset\n",
    "The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__` should return:\n",
    "\n",
    "- image: a PIL Image of size `(H,W)`\n",
    "- target: a dict containing the follwing fields\n",
    "    - `boxes (FloatTensor[N,4]`: the coordinates of `N` bounding boxes in `[x0,y0,x1,y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    - `labels (Int64Tensor[N])`: the label for each bounding box. `0` represents always the background class.\n",
    "    - `image_id (Int64Tensor[1])`: an image identifier. It should be unique between all the images in the dataset, and is used during evalution.\n",
    "    - `area (Tensor[N])`: the area of the bounding box. This is used during evalation with the COCO metric, to separate metric scores between small, medium and large boxes.\n",
    "    - `iscrowd (Uint8Tensor[N,H,W])`: instances with with iscrowd=True will be ignored durining evaluation.\n",
    "    - (optionally) `masks (UInt8Tensor[N, H, W]`: The segmentation masks for each one of the objects.\n",
    "    - (optionally) `keypoints (FloatTensor[N, K, 3]`: For each one of the N objects, it contians the K keypoints in `[x, y, visibility]` format, defining the object. visibility=0 means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt r`eferences/detection/transforms.py` for your new keypoint representation\n",
    "    \n",
    "\n",
    "One note on the `labels`. The model considers class `0` as background. If your dataset does not contain the background class, you should not have `0` in your `labels`.\n",
    "For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has booth classes, your labels tensor should look like [1,2].\n",
    "\n",
    "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n",
    "\n",
    "\n",
    "## 1. Write a custom dataset for PennFudan\n",
    "\n",
    "Let’s write a dataset for the PennFudan dataset. After [downloading and extracting the zip file](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip), we have the following folder structure:\n",
    "\n",
    "```console\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "```\n",
    "\n",
    "Here is on example of pair of images and segmentation masks:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image01.png\">\n",
    "<img src=\"https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image02.png\">\n",
    "\n",
    "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a `torch.utils.data.Dataset` class for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        #load all image files, sorting them to\n",
    "        #ensure that they are alighed\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PNGMasks\"))))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PNGMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        #convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        #instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        #first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        #split the color-encoded mask into a set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
