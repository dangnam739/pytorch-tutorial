{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision Object Detection Finetuning Tutorial\n",
    "\n",
    "**References**: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "\n",
    "For this tutorial, we will be finetuning a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "## Define the Dataset\n",
    "The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement `__len__` and `__getitem__`.\n",
    "\n",
    "The only specificity that we require is that the dataset `__getitem__` should return:\n",
    "\n",
    "- image: a PIL Image of size `(H,W)`\n",
    "- target: a dict containing the follwing fields\n",
    "    - `boxes (FloatTensor[N,4]`: the coordinates of `N` bounding boxes in `[x0,y0,x1,y1]` format, ranging from `0` to `W` and `0` to `H`\n",
    "    - `labels (Int64Tensor[N])`: the label for each bounding box. `0` represents always the background class.\n",
    "    - `image_id (Int64Tensor[1])`: an image identifier. It should be unique between all the images in the dataset, and is used during evalution.\n",
    "    - `area (Tensor[N])`: the area of the bounding box. This is used during evalation with the COCO metric, to separate metric scores between small, medium and large boxes.\n",
    "    - `iscrowd (Uint8Tensor[N,H,W])`: instances with with iscrowd=True will be ignored durining evaluation.\n",
    "    - (optionally) `masks (UInt8Tensor[N, H, W]`: The segmentation masks for each one of the objects.\n",
    "    - (optionally) `keypoints (FloatTensor[N, K, 3]`: For each one of the N objects, it contians the K keypoints in `[x, y, visibility]` format, defining the object. visibility=0 means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt r`eferences/detection/transforms.py` for your new keypoint representation\n",
    "    \n",
    "\n",
    "One note on the `labels`. The model considers class `0` as background. If your dataset does not contain the background class, you should not have `0` in your `labels`.\n",
    "For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has booth classes, your labels tensor should look like [1,2].\n",
    "\n",
    "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n",
    "\n",
    "\n",
    "## 1. Write a custom dataset for PennFudan\n",
    "\n",
    "Let’s write a dataset for the PennFudan dataset. After [downloading and extracting the zip file](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip), we have the following folder structure:\n",
    "\n",
    "```console\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "```\n",
    "\n",
    "Here is on example of pair of images and segmentation masks:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image01.png\">\n",
    "<img src=\"https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image02.png\">\n",
    "\n",
    "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a `torch.utils.data.Dataset` class for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        #load all image files, sorting them to\n",
    "        #ensure that they are alighed\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PNGMasks\"))))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PNGMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        #convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        #instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        #first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        #split the color-encoded mask into a set\n",
    "        #of binary masks\n",
    "        masks =  mask == obj_ids[:, None, None]\n",
    "        \n",
    "        #get bounding box coordinates for each mask \n",
    "        num_objs -= len(obj_ids)\n",
    "        boxes = []\n",
    "        \n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        #convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        #there is only one class\n",
    "        labels = torch.ones((num_objs, ), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.unit8)\n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1])*(boxes[:, 2] - boxes[:, 0])\n",
    "        #suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs, ), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        target[\"area\"] = area\n",
    "        target[\"image_id\"]  = image_id\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, targer = self.transforms(img, target)\n",
    "            \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining model\n",
    "\n",
    "In this tutorial, we will be using [Mask R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a model that predicts both bouding boxes and class scores for potential objects in the image.\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png\">\n",
    "\n",
    "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmantation masks for each instance.\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image04.png\">\n",
    "\n",
    "There are two common situations where one might want to modify one of the available models in torchvision modelzoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\n",
    "\n",
    "\n",
    "Let’s go see how we would do one or another in the following sections.\n",
    "\n",
    "### 2.1 Finetuning from a pretrained model\n",
    "Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/dangnam739/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9646289ab342413d9c786b8a8b71389c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "#load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "#replace the classifierwith a new one, that has num_classes which user-defined\n",
    "num_classes =2 #1 class (person) + background\n",
    "#get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
